<!--
 * @Author: Roger
 * @Date: 2020-07-04 09:01:00
 * @LastEditors: Roger
 * @LastEditTime: 2020-07-05 03:52:43
--> 

## 6.1.1
1. [延时神经网络]([www.baidu.com](https://www.jianshu.com/p/ab539e9a7955))
## 6.1.3
# RNN的梯度消失与梯度爆炸
RNN可以看成是MLP在时间维度的扩展，我们定义每一时刻运算有：  
$\begin{cases}
    O_{t}=W_oS_t+b_o\\
    S_t=tanh(W_xX_t+W_sS_{t-1}+b_s)
\end{cases}$  
而梯度运算其实就是关于$W_o,W_x,W_s,b_o,b_s$求导。  
例如对于时序数据$x_1,x_2,x_3$,有  
$\begin{cases}S_1=tanh(W_xX_1+W_sS_{0}+b_s)\\
O_1=W_oS_1+b_o\\
S_2=tanh(W_xX_2+W_sS_{1}+b_s)\\
O_2=W_oS_2+b_o\\
S_3=tanh(W_xX_3+W_sS_{2}+b_s)\\
O_3=W_oS_3+b_o\end{cases}$  
这里可以随机初始化$S_0$。对输出$O$计算损失函数,并进行梯度回传，总的loss为各时刻loss的和，因此总loss关于各个参数的梯度也是各个时刻关于各个参数的梯度之和，即:  
$L=\sum_{t=1}^TL_t\\$
$\frac{\partial L}{\partial W_o}=\sum_{t=1}^T\frac{\partial L_t}{\partial W_o}\\$
$\frac{\partial L}{\partial W_x}=\sum_{t=1}^T\frac{\partial L_t}{\partial W_x}\\$
$\frac{\partial L}{\partial W_os}=\sum_{t=1}^T\frac{\partial L_t}{\partial W_s}\\$
每一时刻对$W_o$求导不依赖序列前序时刻的状态，有:  
$$\frac{\partial L_t}{\partial W_o}=\frac{\partial L_t}{\partial O_t} \frac{\partial O_t}{\partial W_o} \tag{3.1}$$
关于$W_x$求导有：  
$$\frac{\partial L_t}{\partial W_x}=\frac{\partial L_t}{\partial O_t} \frac{\partial O_t}{\partial S_t}\frac{\partial S_t}{\partial W_x} \tag{3.2}$$
其中第一项与第二项都只与当前时刻t的具体数字相关。而第三项与前序时刻相关，根据链式法则有：  
$$\frac{\partial S_t}{\partial W_x}=\frac{\partial S_t}{\partial W_x}+\frac{\partial S_t}{\partial S_{t-1}}\frac{\partial S_{t-1}}{\partial W_x}+\frac{\partial S_t}{\partial S_{t-1}}\frac{\partial S_{t-1}}{\partial S_{t-2}}\frac{\partial S_{t-2}}{\partial W_x}+...+\frac{\partial S_t}{\partial S_{t-1}}\frac{\partial S_{t-1}}{\partial S_{t-2}}...\frac{\partial S_{1}}{\partial W_x} \tag{3.2.1}$$
将公式${3.2.1}$带入${3.2}$有
$$\frac{\partial L_t}{\partial W_x}=\frac{\partial L_t}{\partial O_t} \frac{\partial O_t}{\partial S_t}\sum_{k=1}^t(\prod_{j=k}^{t} \frac{\partial S_{j+1}}{\partial S_{j}})\frac{\partial S_{k}}{\partial W_x}=\sum_{k=1}^t\frac{\partial L_t}{\partial O_t} \frac{\partial O_t}{\partial S_t}(\prod_{j=k}^{t} \frac{\partial S_{j+1}}{\partial S_{j}})\frac{\partial S_{k}}{\partial W_x} \tag{3.3}$$
关于$W_s,b_s,b_o$等同理，关于矩阵求导的步骤可参考NNDL。
## 梯度消失与梯度爆炸
$\frac{\partial S_{j+1}}{\partial S_{j}}$由$S_t=tanh(W_xX_t+W_sS_{t-1}+b_s)$可知$\frac{\partial S_{j+1}}{\partial S_{j}}=\tanh'W_s$。  
其中，$\tanh'W_s$中前者范围在0~1之间，整体若极小时，t很大时趋于0；整体若极大时，t很大时趋于无穷，也即t很大时刻的梯度随着链条回传，所占比重越来越小或者越来越爆炸。即RNN的 所谓梯度消失的真正含义是，梯度被近距离梯度主导，导致模型难以学到远距离的依赖关系。  
梯度截断可以解决梯度爆炸，lstm可缓解梯度消失。
## LSTM
关键公式为：  
$c_t=f_t\bigodot c_{t-1}+i_t\bigodot g_{t}$  
关于$\frac{\partial c_t}{\partial f_t}$同样包含连乘，但是$f_t$是表示遗忘门，是表示关键信息是否保留的gate,当gate=1时，梯度保留，而gate=0时，梯度为0,同说明信息不需要保留，故
> LSTM不是让所有远距离的梯度值都不会消散，而是只让具有时序关键信息位置的梯度可以一直传递  
具体可参考[lstm](https://www.zhihu.com/question/34878706),[lstm](https://zhuanlan.zhihu.com/p/36101196)








